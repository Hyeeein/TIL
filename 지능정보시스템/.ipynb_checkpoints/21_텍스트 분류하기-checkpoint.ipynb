{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e960688",
   "metadata": {},
   "source": [
    "## 분류기 이용하기\n",
    "\n",
    "\n",
    "### 분류(Classify)와 분류기(Classifier)\n",
    "\n",
    "- 나이브 베이즈 분류기 (Naive Bayes Classifier)\n",
    "- 의사결정 나무(Decision Tree)\n",
    "- 랜덤 포레스트(Random Forest)\n",
    "- 서포터 벡터 머신(SVM, Support Vector Machine)\n",
    "\n",
    "\n",
    "### 나이브 베이즈 분류기\n",
    "\n",
    "* **베이즈 정리**에 기반한 **통계적 분류** 기법\n",
    "    - 베이즈 정리: p(a|b) = p(a|b)/p(b) = p(b|a)p(a)/p(b)\n",
    "    - 이미 알고 있는 값들을 이용하여 계산한 것\n",
    "    - 예) 비가 오는 날 경기를 할 확률은? P(Yes|비) = P(비|Yes)P(Yes) / P(비)\n",
    "    \n",
    "* 추가예시 1\n",
    "    - p(긍정|입력텍스트) = p(입력텍스트|긍정)p(긍정)\n",
    "    - p(부정|입력텍스트) = p(입력텍스트|부정)p(부정)\n",
    "    \n",
    "* Naive Bayes Classifier의 'Naive'\n",
    "    - 단순한, 순진한\n",
    "    - 개별 단ㅇ끼리는 서로 조건부 독립이 성립한다는 순진한 믿음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d9eb6",
   "metadata": {},
   "source": [
    "### 데이터 입력하기: 영어 버전\n",
    "\n",
    "\n",
    "\n",
    "* 분류기: **NLTK 이용**\n",
    "\n",
    "\n",
    "* *NLTK란?*\n",
    "    - Natural Language Toolkit (자연어 처리 툴킷 / 패키지)\n",
    "    - 교육용으로 개발된 자연어 처리 및 텍스트 분석을 위한 패키지\n",
    "    - word_tokenize와 NaiveBayesClassifier를 사용할 예정\n",
    "    - 공식 홈페이지: https://www.nltk,org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313caa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 불러오기\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90d690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\phi49\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e1d87",
   "metadata": {},
   "source": [
    "* **데이터 입력하기 (영어 버전)**\n",
    "    - 이미 알고 있는 정보를 튜플(tuple) 형태로 입력\n",
    "    - 어떤 단어가 긍정/부정을 결정하는지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f8b9228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I like you', 'pos'),\n",
       " ('I hate you', 'neg'),\n",
       " ('I enjoyed you', 'pos'),\n",
       " ('I hate it', 'neg')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [('I like you', 'pos'), ('I hate you', 'neg'),\n",
    "        ('I enjoyed you', 'pos'), ('I hate it', 'neg')]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38311817",
   "metadata": {},
   "source": [
    "* **확률을 구하기 위한 학습 과정**\n",
    "    - 1) 모든 단어 집합 만들기\n",
    "    - 2) 각 문장을 단어가 있는지 없는지 여부로 표현\n",
    "    - 3) 단어별 확률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8196de99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I', 'enjoyed', 'hate', 'it', 'like', 'you'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train에 있는 문장들의 단어 set 만들기\n",
    "# set()을 이용하여 집합으로 만들기\n",
    "\n",
    "all_words = set()\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        all_words.add(word)   # 집합은 .add 사용\n",
    "        \n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c6080f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'like': True, 'I': True, 'you': True, 'hate': False, 'it': False, 'enjoyed': False}, 'pos')\n",
      "({'like': False, 'I': True, 'you': True, 'hate': True, 'it': False, 'enjoyed': False}, 'neg')\n",
      "({'like': False, 'I': True, 'you': True, 'hate': False, 'it': False, 'enjoyed': True}, 'pos')\n",
      "({'like': False, 'I': True, 'you': False, 'hate': True, 'it': True, 'enjoyed': False}, 'neg')\n"
     ]
    }
   ],
   "source": [
    "# train에 있는 문장들의 단어 set 만들기\n",
    "# set()을 이용하여 집합으로 만들기\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    \n",
    "    # 각 문장을 단어가 있는지 없는지 여부로 표현\n",
    "    tmp = dict()\n",
    "    for set_word in all_words:\n",
    "        if set_word in words:\n",
    "            tmp[set_word] = True\n",
    "        else:\n",
    "            tmp[set_word] = False\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "\n",
    "# [('I like you', 'pos'), ('I hate you', 'neg'), ('I enjoyed you', 'pos'), ('I hate it', 'neg')]\n",
    "# 단어의 등장 순서를 무시 -> 빈도만을 이용함\n",
    "print(train_features[0])\n",
    "print(train_features[1])\n",
    "print(train_features[2])\n",
    "print(train_features[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad416ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'like': True, 'I': True, 'you': True, 'hate': False, 'it': False, 'enjoyed': False}, 'pos')\n",
      "({'like': False, 'I': True, 'you': True, 'hate': True, 'it': False, 'enjoyed': False}, 'neg')\n",
      "({'like': False, 'I': True, 'you': True, 'hate': False, 'it': False, 'enjoyed': True}, 'pos')\n",
      "({'like': False, 'I': True, 'you': False, 'hate': True, 'it': True, 'enjoyed': False}, 'neg')\n"
     ]
    }
   ],
   "source": [
    "### 간략하게 작성하기 (1)\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = dict()\n",
    "    for set_word in all_words:\n",
    "        tmp[set_word] = (set_word in words)  # set_word가 words에 있는지 여부 판별\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "\n",
    "# [('I like you', 'pos'), ('I hate you', 'neg'), ('I enjoyed you', 'pos'), ('I hate it', 'neg')]\n",
    "print(train_features[0])\n",
    "print(train_features[1])\n",
    "print(train_features[2])\n",
    "print(train_features[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6858f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'like': True, 'I': True, 'you': True, 'hate': False, 'it': False, 'enjoyed': False}, 'pos')\n",
      "({'like': False, 'I': True, 'you': True, 'hate': True, 'it': False, 'enjoyed': False}, 'neg')\n",
      "({'like': False, 'I': True, 'you': True, 'hate': False, 'it': False, 'enjoyed': True}, 'pos')\n",
      "({'like': False, 'I': True, 'you': False, 'hate': True, 'it': True, 'enjoyed': False}, 'neg')\n"
     ]
    }
   ],
   "source": [
    "### 간략하게 작성하기 (2)\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = {set_word: (set_word in words) for set_word in all_words}  # for문 다음에 오는 명령문을 1줄로 작성\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "\n",
    "# [('I like you', 'pos'), ('I hate you', 'neg'), ('I enjoyed you', 'pos'), ('I hate it', 'neg')]\n",
    "print(train_features[0])\n",
    "print(train_features[1])\n",
    "print(train_features[2])\n",
    "print(train_features[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdd895",
   "metadata": {},
   "source": [
    "* 단어별 확률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc7593be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 enjoyed = False             neg : pos    =      1.7 : 1.0\n",
      "                      it = False             pos : neg    =      1.7 : 1.0\n",
      "                    like = False             neg : pos    =      1.7 : 1.0\n",
      "                     you = True              pos : neg    =      1.7 : 1.0\n",
      "                       I = True              neg : pos    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c795c3f",
   "metadata": {},
   "source": [
    "`결과 해석: 'like'라는 단어가 문장에 없으면(False), neg:pos 비율이 1.7:1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba442c",
   "metadata": {},
   "source": [
    "* 테스트하기\n",
    "    - 문장에 사용딘 단어 중 한 개의 단어라도 학습 데이터에 없었다면? 전체 확률이 0이 되어버림\n",
    "    - *라플라스 스무딩(Laplace smoothing)* 이용: 분모, 분자에 일정한 값을 더해서 분자가 0이 되는 것을 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b04e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': True, 'I': True, 'you': False, 'hate': False, 'it': True, 'enjoyed': False}\n"
     ]
    }
   ],
   "source": [
    "test_sent = 'I like it'\n",
    "\n",
    "words = word_tokenize(test_sent)\n",
    "test_feature = {set_word: (set_word in words) for set_word in all_words}\n",
    "\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa7f029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844e562",
   "metadata": {},
   "source": [
    "### 데이터 입력하기 : 한국어 버전\n",
    "\n",
    "\n",
    "* 이미 알고 잇는 정보를 튜플(tuple) 형태로 입력\n",
    "\n",
    "\n",
    "* 어떤 단어가 긍정/부정을 결정하는지?\n",
    "    - '사과가 좋아' -> 긍정\n",
    "    - '밤에 먹는 사과가 비추야' - > 부정\n",
    "    - '사과가 잘 익었어 맛있겠다' -> 긍정\n",
    "    - '바나나는 맛있어' -> 긍정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06f294c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('사과가 좋아', 'pos'),\n",
       " ('밤에 먹는 사과는 비추야', 'neg'),\n",
       " ('사과가 잘 익었어 맛있겠다', 'pos'),\n",
       " ('바나나는 맛있어', 'pos')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 입력해주기\n",
    "\n",
    "train = [('사과가 좋아', 'pos'), ('밤에 먹는 사과는 비추야', 'neg'), \n",
    "         ('사과가 잘 익었어 맛있겠다', 'pos'), ('바나나는 맛있어', 'pos')]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b8985",
   "metadata": {},
   "source": [
    "* 모든 단어 집합 만들기\n",
    "* 각 문장을 단어가 있는지 없는지 여부로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb6fa4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'맛있겠다', '맛있어', '먹는', '바나나는', '밤에', '비추야', '사과가', '사과는', '익었어', '잘', '좋아'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train에 있는 문장들의 단어 set 만들기\n",
    "# set()을 이용하여 집합으로 만들기\n",
    "\n",
    "all_words = set()\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        all_words.add(word)\n",
    "\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf5af3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'먹는': False, '비추야': False, '잘': False, '맛있겠다': False, '좋아': True, '맛있어': False, '밤에': False, '바나나는': False, '익었어': False, '사과가': True, '사과는': False}, 'pos')\n",
      "({'먹는': True, '비추야': True, '잘': False, '맛있겠다': False, '좋아': False, '맛있어': False, '밤에': True, '바나나는': False, '익었어': False, '사과가': False, '사과는': True}, 'neg')\n",
      "({'먹는': False, '비추야': False, '잘': True, '맛있겠다': True, '좋아': False, '맛있어': False, '밤에': False, '바나나는': False, '익었어': True, '사과가': True, '사과는': False}, 'pos')\n",
      "({'먹는': False, '비추야': False, '잘': False, '맛있겠다': False, '좋아': False, '맛있어': True, '밤에': False, '바나나는': True, '익었어': False, '사과가': False, '사과는': False}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = {set_word: (set_word in words) for set_word in all_words}\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "    \n",
    "for i in range(len(train_features)):\n",
    "    print(train_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37eb488",
   "metadata": {},
   "source": [
    "* 단어별 확률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a98efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     사과가 = False             neg : pos    =      2.0 : 1.0\n",
      "                    맛있겠다 = False             neg : pos    =      1.2 : 1.0\n",
      "                     맛있어 = False             neg : pos    =      1.2 : 1.0\n",
      "                    바나나는 = False             neg : pos    =      1.2 : 1.0\n",
      "                     익었어 = False             neg : pos    =      1.2 : 1.0\n",
      "                       잘 = False             neg : pos    =      1.2 : 1.0\n",
      "                      좋아 = False             neg : pos    =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342632dc",
   "metadata": {},
   "source": [
    "* 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c55cb4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'먹는': False, '비추야': False, '잘': False, '맛있겠다': False, '좋아': False, '맛있어': True, '밤에': False, '바나나는': False, '익었어': False, '사과가': False, '사과는': True}\n"
     ]
    }
   ],
   "source": [
    "test_sent = '사과는 맛있어'\n",
    "\n",
    "words = word_tokenize(test_sent)\n",
    "test_feature = {set_word: (set_word in words) for set_word in all_words}\n",
    "\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aca0705d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a289e",
   "metadata": {},
   "source": [
    "### 학습하기 (1) : 형태소 분석기 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6004b",
   "metadata": {},
   "source": [
    "### 함수 만들기\n",
    "\n",
    "- 문장을 형태소 단위로 분리해주는 함수 작성\n",
    "\n",
    "1) 처음 문장 (raw_sent): '사과가 좋아' <br>\n",
    "2) 형태소 분석 결과 문장 (sent): [('사과', 'Noun'), ('가', 'Josa'), ('좋다', 'Adjective')] <br>\n",
    "3) 리턴 문장 (''join(pos_sent): '사과/Noun 가/Josa 좋다/Adjective'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd890459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "320446ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tokenize(raw_sent):\n",
    "    pos_sent = []\n",
    "    \n",
    "    # raw_sent = '사과가 좋아'\n",
    "    sent = okt.pos(raw_sent, norm=True, stem=True)\n",
    "    # sent: [('사과', 'Noun'), ('가', 'Josa'), ('좋다', 'Adjective')]\n",
    "    \n",
    "    for tup in sent:\n",
    "        word, tag = tup[0], tup[1]    # tup: ('사과', 'Noun')\n",
    "        word_tag = word + '/' + tag   # word_tag: '사과/Noun'\n",
    "        pos_sent.append(word_tag)\n",
    "        \n",
    "    return ' '.join(pos_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7fd39da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'자다/Verb', '익다/Verb', '비추다/Verb', '사과/Noun', '에/Josa', '가/Josa', '먹다/Verb', '바나나/Noun', '밤/Noun', '맛있다/Adjective', '는/Josa', '좋다/Adjective'}\n"
     ]
    }
   ],
   "source": [
    "# train에 있는 문자들의 단어 set 만들기\n",
    "# set()을 이용하여 집합으로 만들기\n",
    "\n",
    "all_words = set()\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    sent = pos_tokenize(sent)    # pos_tokenize 함수 추가\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        all_words.add(word)\n",
    "\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f698aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'자다/Verb': False, '익다/Verb': False, '비추다/Verb': False, '사과/Noun': True, '에/Josa': False, '가/Josa': True, '먹다/Verb': False, '바나나/Noun': False, '밤/Noun': False, '맛있다/Adjective': False, '는/Josa': False, '좋다/Adjective': True}, 'pos')\n",
      "({'자다/Verb': False, '익다/Verb': False, '비추다/Verb': True, '사과/Noun': True, '에/Josa': True, '가/Josa': False, '먹다/Verb': True, '바나나/Noun': False, '밤/Noun': True, '맛있다/Adjective': False, '는/Josa': True, '좋다/Adjective': False}, 'neg')\n",
      "({'자다/Verb': True, '익다/Verb': True, '비추다/Verb': False, '사과/Noun': True, '에/Josa': False, '가/Josa': True, '먹다/Verb': False, '바나나/Noun': False, '밤/Noun': False, '맛있다/Adjective': True, '는/Josa': False, '좋다/Adjective': False}, 'pos')\n",
      "({'자다/Verb': False, '익다/Verb': False, '비추다/Verb': False, '사과/Noun': False, '에/Josa': False, '가/Josa': False, '먹다/Verb': False, '바나나/Noun': True, '밤/Noun': False, '맛있다/Adjective': True, '는/Josa': True, '좋다/Adjective': False}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    sent = pos_tokenize(sent)    # pos_tokenize 함수 추가\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = {set_word: (set_word in words) for set_word in all_words}\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "    \n",
    "for i in range(len(train_features)):\n",
    "    print(train_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43ef8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  가/Josa = False             neg : pos    =      2.0 : 1.0\n",
      "                  는/Josa = True              neg : pos    =      2.0 : 1.0\n",
      "           맛있다/Adjective = False             neg : pos    =      2.0 : 1.0\n",
      "                바나나/Noun = False             neg : pos    =      1.2 : 1.0\n",
      "                 사과/Noun = True              neg : pos    =      1.2 : 1.0\n",
      "                 익다/Verb = False             neg : pos    =      1.2 : 1.0\n",
      "                 자다/Verb = False             neg : pos    =      1.2 : 1.0\n",
      "            좋다/Adjective = False             neg : pos    =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f481bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'자다/Verb': False, '익다/Verb': False, '비추다/Verb': False, '사과/Noun': True, '에/Josa': False, '가/Josa': False, '먹다/Verb': False, '바나나/Noun': False, '밤/Noun': False, '맛있다/Adjective': True, '는/Josa': True, '좋다/Adjective': False}\n"
     ]
    }
   ],
   "source": [
    "# 테스트하기\n",
    "test_sent = '사과는 맛있어'\n",
    "\n",
    "test_sent = pos_tokenize(test_sent)  # pos_tokenize 함수 추가\n",
    "words = word_tokenize(test_sent)  \n",
    "test_feature = {set_word: (set_word in words) for set_word in all_words}\n",
    "\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59fcfe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
