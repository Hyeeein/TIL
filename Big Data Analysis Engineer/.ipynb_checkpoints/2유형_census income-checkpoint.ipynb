{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb9c33f",
   "metadata": {},
   "source": [
    "## 2유형 [3] Census Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c7404ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.86711\n",
      "[10]\tvalidation_0-auc:0.90872\n",
      "[20]\tvalidation_0-auc:0.91014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phi49\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\phi49\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalidation_0-auc:0.91107\n",
      "[40]\tvalidation_0-auc:0.91134\n",
      "[50]\tvalidation_0-auc:0.91115\n",
      "[60]\tvalidation_0-auc:0.91123\n",
      "[70]\tvalidation_0-auc:0.91157\n",
      "[80]\tvalidation_0-auc:0.91159\n",
      "[90]\tvalidation_0-auc:0.91160\n",
      "[100]\tvalidation_0-auc:0.91155\n",
      "[110]\tvalidation_0-auc:0.91197\n",
      "[120]\tvalidation_0-auc:0.91175\n",
      "[130]\tvalidation_0-auc:0.91179\n",
      "[140]\tvalidation_0-auc:0.91176\n",
      "[150]\tvalidation_0-auc:0.91171\n",
      "[158]\tvalidation_0-auc:0.91170\n"
     ]
    }
   ],
   "source": [
    "# 출력을 원하실 경우 print() 함수 활용\n",
    "# 예시) print(df.head())\n",
    "\n",
    "# getcwd(), chdir() 등 작업 폴더 설정 불필요\n",
    "# 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가\n",
    "\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_test = pd.read_csv(\"./datasets/Part2/census_X_test.csv\")\n",
    "X_train = pd.read_csv(\"./datasets/Part2/census_X_train.csv\")\n",
    "y_train = pd.read_csv(\"./datasets/Part2/census_y_train.csv\")\n",
    "\n",
    "# 사용자 코딩\n",
    "\n",
    "### 1. 데이터 전처리\n",
    "# 1) 데이터 확인 -> 결측치는 따로 없음\n",
    "# print(X_train.info())\n",
    "\n",
    "# 2) 기술통계량\n",
    "# print(X_train.describe())\n",
    "\n",
    "# 3) 기술통계량에서 capital_gain, capital_loss 이상치 발견 (오른쪽으로 치우침) => 백분위수 쪼개서 살펴보기\n",
    "# print(X_train['capital_gain'].quantile([q/20 for q in range(15, 21)]))\n",
    "# print(X_train['capital_loss'].quantile([q/20 for q in range(15, 21)]))\n",
    "\n",
    "# 4) 범주형 변수로 바꾸어 파생변수를 생성하고, 기존변수와 중요도 확인 (더 높은 것 사용)\n",
    "X_train['capital_gain_yn'] = np.where(X_train['capital_gain']>0, 1, 0)\n",
    "X_train['capital_loss_yn'] = np.where(X_train['capital_loss']>0, 1, 0)\n",
    "\n",
    "X_test['capital_gain_yn'] = np.where(X_test['capital_gain']>0, 1, 0)\n",
    "X_test['capital_loss_yn'] = np.where(X_test['capital_loss']>0, 1, 0)\n",
    "\n",
    "# 5) 종속변수 값별로 각 독립변수의 분포 확인 (숫자형)\n",
    "# 컬럼명 저장\n",
    "COL_DEL = []\n",
    "COL_NUM = ['age', 'education_num', 'hours_per_week', 'capital_gain', 'capital_loss']\n",
    "COL_CAT = ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "COL_Y = ['target']\n",
    "\n",
    "X_train = X_train.drop(COL_DEL, axis=1)\n",
    "X_test = X_test.drop(COL_DEL, axis=1)\n",
    "\n",
    "# 기술통계량 확인\n",
    "# 종속변수의 값이 1일 때의 평균이 0일 때의 평균보다 크므로 예측에 사용할 수 있는 데이터 확인\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# for col in COL_NUM:\n",
    "#     print('-' * 80)\n",
    "#     print(col)\n",
    "#     print(train_df.groupby(COL_Y)[col].describe(), end='\\n\\n')\n",
    "\n",
    "# 6) 각 범주형 변수별로 종속변수의 평균을 확인 (문자형)\n",
    "# for col in COL_CAT:\n",
    "#     print(col)\n",
    "#     print(train_df.groupby(col, as_index=False)[COL_Y].mean().sort_values(by=COL_Y, ascending=False), end='\\n\\n')\n",
    "\n",
    "# 7) 레이블 인코딩 (범주형 변수들 -> 숫자로 변경)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "for col in COL_CAT:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train[col])\n",
    "    X_train[col] = le.transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "### --------------------\n",
    "\n",
    "### 2. 모델 구축\n",
    "# 1) 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train)\n",
    "\n",
    "# 2) 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr[COL_NUM] = scaler.fit_transform(X_tr[COL_NUM])\n",
    "X_val[COL_NUM] = scaler.fit_transform(X_val[COL_NUM])\n",
    "X_test[COL_NUM] = scaler.fit_transform(X_test[COL_NUM])\n",
    "\n",
    "# 3) 분류모델 사용: 랜덤포레스트, XGBoost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_tr, y_tr.values.ravel())\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# n_estimators=100\n",
    "model_xgb1 = XGBClassifier()\n",
    "model_xgb1.fit(X_tr, y_tr.values.ravel())\n",
    "\n",
    "# XGBoost 학습 시, eval_set을 사용하여 과적합 방지 가능\n",
    "# early_stopping_rounds 이상의 iteration 동안 성능 개선이 없으면 학습을 멈춤\n",
    "# model_xgb2 = XGBClassifier(n_estimators=1000, learning_rate=0.1, max_depth=10)\n",
    "# model_xgb2.fit(X_tr, y_tr.values.ravel(), early_stopping_rounds=50, eval_metric='auc', eval_set=[(X_val, y_val)], verbose=10)\n",
    "\n",
    "### --------------------\n",
    "\n",
    "### 3. 모델 평가 (ROC_AUC_SCORE) & 하이퍼 파라미터 튜닝\n",
    "# 1) ROC-AUC 값 구하기\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_rf = model_rf.predict_proba(X_val)\n",
    "y_pred_xgb1 = model_xgb1.predict_proba(X_val)\n",
    "\n",
    "score_rf = roc_auc_score(y_val, y_pred_rf[:, 1])\n",
    "score_xgb1 = roc_auc_score(y_val, y_pred_xgb1[:, 1])\n",
    "\n",
    "# print(score_rf) : 0.8894481775603122\n",
    "# print(score_xgb1) : 0.9123917379702831 -> XGBoost가 성능이 더 좋음\n",
    "\n",
    "# 2) 하이퍼 파라미터 튜닝 전, 파생변수의 변수 중요도 확인\n",
    "# print(pd.DataFrame({'feature': X_tr.columns, 'fi_rf': model_rf.feature_importances_, \n",
    "#                     'fi_xgb': model_xgb1.feature_importances_}))\n",
    "\n",
    "# 중요도가 너무 낮으므로 해당 컬럼 제외\n",
    "COL_DEL = ['capital_gain_yn', 'capital_loss_yn']\n",
    "\n",
    "X_tr = X_tr.drop(COL_DEL, axis=1)\n",
    "X_val = X_val.drop(COL_DEL, axis=1)\n",
    "X_test = X_test.drop(COL_DEL, axis=1)\n",
    "\n",
    "# 3) 하이퍼 파라미터 튜닝 (랜덤 포레스트)\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid_params = { \n",
    "#     'max_depth': [5, 10, 15],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# rf_cv = GridSearchCV(estimator=model_rf, param_grid=grid_params, cv=5)\n",
    "# rf_cv.fit(X_train, y_train.values.ravel()) \n",
    "\n",
    "# print(pd.DataFrame(rf_cv.cv_results_).head())\n",
    "# print(rf_cv.best_params_)\n",
    "\n",
    "# 4) 하이퍼 파라미터 튜닝 (XGBoost)\n",
    "# model_rf2 = RandomForestClassifier(n_estimators=50\n",
    "#                                    , max_depth=15\n",
    "#                                    , min_samples_leaf=1\n",
    "#                                    , min_samples_split=5)\n",
    "# model_rf2.fit(X_tr, y_tr.values.ravel())\n",
    "\n",
    "# y_pred_rf2 = model_rf2.predict_proba(X_val)\n",
    "# score_rf2 = roc_auc_score(y_val, y_pred_rf2[:, 1])\n",
    "# print(score_rf2)\n",
    "\n",
    "# grid_params = {'max_depth': [3, 5, 7, 10], \n",
    "#                'min_child_weight': [1, 2], \n",
    "#                'colsample_bytree': [0.6, 0.8],\n",
    "#                'subsample': [0.6, 0.8]}\n",
    "\n",
    "# xgb_cv = GridSearchCV(estimator=model_xgb1, param_grid=grid_params, cv=5)\n",
    "# xgb_cv.fit(X_tr, y_tr.values.ravel())\n",
    "\n",
    "# print(xgb_cv.best_params_)\n",
    "\n",
    "# 5) 가장 좋은 하이퍼 파라미터로 재학습\n",
    "params = {'colsample_bytree': 0.6,\n",
    "          'max_depth': 7,\n",
    "          'min_child_weight': 1,\n",
    "          'subsample': 0.8}\n",
    "\n",
    "model_xgb3 = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "model_xgb3.set_params(**params)\n",
    "\n",
    "model_xgb3.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='auc', eval_set=[(X_val, y_val)], verbose=10)\n",
    "\n",
    "# print(model_xgb3.best_score)\n",
    "\n",
    "### --------------------\n",
    "\n",
    "### 4. 제출 파일로 변환\n",
    "\n",
    "# 1) 모델 예측 값 계산\n",
    "pred = model_xgb3.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2) 파일 저장\n",
    "pd.DataFrame({'index': X_test.index, 'target': pred}).to_csv('./res/003000000.csv', index=False)\n",
    "\n",
    "# 답안 제출 참고\n",
    "# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n",
    "# pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigbunki",
   "language": "python",
   "name": "bigbunki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
